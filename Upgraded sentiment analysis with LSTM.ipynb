{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86418970-3792-4015-9f44-d5ac8173bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "SEED=42\n",
    "torch.manual_seed(SEED)\n",
    "import re\n",
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a21856f-c903-4e28-afc7-1029002e5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to build vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx = None, add_unk = True, unk_token = \"<UNK>\", mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",end_seq_token=\"<END>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx: Initialize token to idx dictionary.\n",
    "            add_unk: Whether to include the unknown token in the vocabulary\n",
    "            unk_token: How the unknown token is represented in the vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {idx:token \n",
    "                             for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "#         self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "#         self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        \n",
    "    def from_serializable(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A dictionary that can be serialized\n",
    "        \"\"\"\n",
    "\n",
    "        return cls(**contents)\n",
    "        \n",
    "    def add_token(self, token):\n",
    "        \"\"\" Update mapping dictionaries given the token\n",
    "        Args:\n",
    "            token (str): Token to add to the vocabulary\n",
    "        Returns:\n",
    "            index (int): The index corresponding to the token          \n",
    "        \"\"\"\n",
    "\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "\n",
    "        return index\n",
    "        \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" Retrieves the index associated with the token\n",
    "            or the UNK index if the token isn't present in the vocabulary\n",
    "\n",
    "        Args:\n",
    "            token (str): The token for which the index has to be retrieved\n",
    "        Returns:\n",
    "            index (int): The index associated with the token in the dictionary\n",
    "\n",
    "        Note: \n",
    "            'UNK Index' has to be >=0 for the UNK functionality\n",
    "        \"\"\"\n",
    "\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx.get(token)\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Retrieve the token associated with the index\n",
    "        Args:\n",
    "            index (int): The index to look up\n",
    "        Returns:\n",
    "            token (str): The token associated with the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the vocabulary\n",
    "        \"\"\"\n",
    "\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index %d is not in the vocabulary\" % index)\n",
    "        else:\n",
    "            return self._idx_to_token[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the vocabulary\n",
    "        \"\"\"\n",
    "        return len(self._token_to_idx)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b707ab-ad83-4e67-888f-6f238c60f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hate_Speech_Tweets(Dataset):\n",
    "    def __init__(self, df, nlp, vocab):\n",
    "        \"\"\"Initializing\n",
    "        Args:\n",
    "            df (Pandas DataFrame): Dataframe consisting of tweets and labels\n",
    "            nlp (spacy object): For preprocessing\n",
    "            vocab (Vocabulary Object): To vectorize the tweets\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.nlp = nlp\n",
    "        self.vocab = vocab\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, self.df.tweet)) + 1\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "          \n",
    "        tweet =  self.df['tweet'].iloc[idx]\n",
    "        \n",
    "        label = self.df['label'].iloc[idx]\n",
    "        return {'tweet':torch.LongTensor(self.preprocess(tweet)), 'label':label}\n",
    "    \n",
    "    def preprocess(self, sent):\n",
    "        \n",
    "        #Preprocessing and tokenizing\n",
    "        sent =  re.sub(r'RT','',sent)\n",
    "        sent =  re.sub('@[a-zA-z0-9_]+','',sent)\n",
    "        sent =  re.sub(r'http://[^\\s<>\"]+|www\\.[^\\s<>\"]+','',sent)\n",
    "        sent =  re.sub(r'&#[0-9]+','',sent)\n",
    "        sent =  re.sub(r'[0-9]+','',sent)\n",
    "        sent =  re.sub(r'[^\\w\\s]', '',sent)\n",
    "        sent =  sent.strip()\n",
    "        sent =  \" \".join(sent.split())\n",
    "        sent = [token.lemma_ for token in self.nlp(sent) if token.text not in STOP_WORDS]\n",
    "        sent = self.vectorize(sent)\n",
    "        return sent\n",
    "    \n",
    "    def vectorize(self, sent):\n",
    "        \"\"\"Converts raw text to numeric vectors using the vocabulary\n",
    "        Args:\n",
    "            sent (str): The tweet to be vectorized\n",
    "        Returns:\n",
    "            vector (list): The vector associated with the tweet\n",
    "        \"\"\"\n",
    "        vector = [self.vocab.begin_seq_index]\n",
    "#         vector = []\n",
    "        for token in sent:\n",
    "            vector.append(vocab.lookup_token(token))\n",
    "        vector.append(self.vocab.end_seq_index)\n",
    "\n",
    "#         out_vector = np.zeros(self._max_seq_length, dtype=np.int64)\n",
    "#         out_vector[:len(vector)] = vector\n",
    "#         out_vector[len(vector):] = self.vocab.mask_index\n",
    "            \n",
    "        return vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1d20f3-250d-4156-8916-e50eb042bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(token_to_idx=None, add_unk=True)\n",
    "nlp = spacy.load(name='en_core_web_sm')\n",
    "\n",
    "def clean(t):\n",
    "    t = re.sub(r'RT','',t)\n",
    "    t = re.sub('@[a-zA-z0-9_]+','',t)\n",
    "    t = re.sub(r'http://[^\\s<>\"]+|www\\.[^\\s<>\"]+','',t)\n",
    "    t = re.sub(r'&#[0-9]+','',t)\n",
    "    t = re.sub(r'[0-9]+','',t)\n",
    "    t = re.sub(r'[^\\w\\s]+', '',t)\n",
    "    t = t.strip()\n",
    "    t = \" \".join(t.split())\n",
    "    t = [token.lemma_ for token in nlp(t) if token.text not in STOP_WORDS]\n",
    "    return t\n",
    "\n",
    "cleaned_tweets = df['tweet'].apply(clean)\n",
    "text = []\n",
    "\n",
    "for i in range(len(cleaned_tweets)):\n",
    "    for word in cleaned_tweets[i]:\n",
    "        text.append(word)\n",
    "\n",
    "from collections import Counter\n",
    "count_dict = Counter(text).most_common(6000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d91208-419b-4b30-8685-b5ce6fa6af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in count_dict:\n",
    "    vocab.add_token(tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130542d1-acd4-4ebd-9d02-e02aa45a629e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c194eed4-6411-4696-a150-a1d366aabe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Hate_Speech_Tweets(df,nlp,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1344b741-d94b-4a2e-b945-bbb661696cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = []\n",
    "\n",
    "# for i in range(len(data)):\n",
    "#     for word in data[i]['tweet']:\n",
    "#         text.append(word)\n",
    "\n",
    "# text = \" \".join(text)\n",
    "\n",
    "# x =  ''.join(text)\n",
    "\n",
    "# plt.figure(figsize = (5,5))\n",
    "# wc = WordCloud(width=3500, height=3500).generate(x)\n",
    "# plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1b6bd21-5c24-456a-ac36-dabcd9927239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6003"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac04bdc4-c96b-4a75-b608-627b8ee6db02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(filepath):\n",
    "    \"\"\"Loads the glove embeddings\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): path to the glove embeddings file\n",
    "    Return:\n",
    "        word_to_index (dict): Mappings from word to index\n",
    "        embeddings (np.array): Embeddings of the words in the vocabulary\n",
    "    \"\"\"\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            # line = word num1 num2 .......\n",
    "            line = line.split(\" \")\n",
    "            word_to_index[line[0]] = index\n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "    \n",
    "    return word_to_index, np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170052d9-6a80-4e7d-897b-97e5f0c6a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_matrix(filepath, words):\n",
    "    \"\"\"Create embedding matrix for a specific set of words\n",
    "    Args:\n",
    "        word_to_index (dict) : mapping of word to index\n",
    "        embeddings (list): embeddings of words\n",
    "        words (list): List of words in the dictionary\n",
    "    Returns:\n",
    "        final_embeddings (np..array) : embedding matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    word_to_idx, embeddings = load_glove(filepath)\n",
    "    embedding_size = embeddings.shape[1]\n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.zeros(embedding_size)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "            \n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9051ec5-49d6-4179-9086-09474cdf3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for idx in range(0, vocab.__len__()):\n",
    "    words.append(vocab.lookup_index(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9073dc1-7ff1-4666-9ace-b0c5015d9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = make_embedding_matrix(r\"C:\\Users\\win10\\Documents\\glove.6B\\glove.6B.50d.txt\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5b51c4d-82c4-4e12-9cb4-c0d789558499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<UNK>',\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "words[i], embs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "interim-completion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([   1,   99,   69, 1371,  163,  184,    5,   28,  223,   61,  468, 1068,\n",
       "          404, 2193,  313,    2]),\n",
       " 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.randint(0,4200)\n",
    "print(i)\n",
    "data[i]['tweet'], data[i]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "violent-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iterator, test_iterator = BucketIterator.splits(\n",
    "#     (train_data,test_data),\n",
    "#     batch_size = 8,\n",
    "#     sort = False,\n",
    "#     device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "broke-accused",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6003, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51611d8d-c79b-42be-ad31-ec3e07bf1c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6003"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tracked-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size, batch_size, embedding_weights, bidirectional = False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embedding_weights, freeze=False, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            bidirectional = bidirectional,batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim*2, label_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, label_size)\n",
    "#         self.act = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, sentences, train = True):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "#         packed_embedded = nn.utils.rnn.pack_padded_sequence(embeds, src_len.cpu(), enforce_sorted=False)\n",
    "        packed_outputs, (hidden,cell) = self.lstm(embeds)\n",
    "#         hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "#         print(hidden.shape)\n",
    "#         print(hidden)\n",
    "        dense_outputs = self.fc(hidden)\n",
    "        outputs = dense_outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "foster-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "nlabel = 3\n",
    "hidden_dim = 50\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = LSTMClassifier(embedding_dim=embs.shape[1],hidden_dim=hidden_dim,label_size=nlabel, batch_size=BATCH_SIZE, embedding_weights=torch.from_numpy(embs).float())\n",
    "model = model.to(device)\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    " \n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cc2ae9b-8b39-4eaf-a83c-74e860f0bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rewrite collate_ FN function, whose input is the sample data of a batch\n",
    "def collate_fn(batch):\n",
    "\t#Because token_ List is a variable length data, so you need to use a list to load the token of the batch_ list\n",
    "    token_lists = [item['tweet'] for item in batch]\n",
    "    #Each label is an int. we take out all the labels in the batch and reassemble them\n",
    "    labels = [item['label'] for item in batch]\n",
    "    #Converting labels to tensor\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return {\n",
    "    'token_list': torch.nn.utils.rnn.pad_sequence(token_lists, batch_first=True),\n",
    "    'label': labels,\n",
    "    }\n",
    "\n",
    "\n",
    "#When using dataloader to load data, pay attention to collate_ The FN parameter passes in an overridden function\n",
    "trainset = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84e18c57-fa8e-4b15-bd28-ac889dc96fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,   67,  107,  123,   26, 1689,  116,    8,    2,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   1, 1237,    0,    0,  564,    0,   37,   34,    0,    2,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   1,  195,  376,  176,   13,  220,  158, 2499,  211,  176, 1187,  241,\n",
      "          123, 2397,  151,    2],\n",
      "        [   1, 1100,  355, 1959,    2,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])\n",
      "tensor([2, 2, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in trainset:\n",
    "    print(batch['token_list'])\n",
    "    print(batch['label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "opponent-optics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:30<00:00, 34.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 1 = 0.79414727111355\n",
      "accuracy on epoch 1 = 0.6207580082650189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:27<00:00, 37.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 2 = 0.5272630900676255\n",
      "accuracy on epoch 2 = 0.7965429749634014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:26<00:00, 40.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 3 = 0.410305385314793\n",
      "accuracy on epoch 3 = 0.8498255629748191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:30<00:00, 34.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 4 = 0.3283729544768977\n",
      "accuracy on epoch 4 = 0.8800348874277211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:29<00:00, 35.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 5 = 0.25179480255973996\n",
      "accuracy on epoch 5 = 0.9133365049348572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:29<00:00, 36.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 6 = 0.19364922524179082\n",
      "accuracy on epoch 6 = 0.936964795432921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:28<00:00, 37.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 7 = 0.15067251934714218\n",
      "accuracy on epoch 7 = 0.9523469711575022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:31<00:00, 33.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 8 = 0.1193287891803662\n",
      "accuracy on epoch 8 = 0.964954012070918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:28<00:00, 37.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 9 = 0.10106188634156403\n",
      "accuracy on epoch 9 = 0.9676498572787822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:29<00:00, 35.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 10 = 0.08678261530804904\n",
      "accuracy on epoch 10 = 0.9757373929590866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:28<00:00, 36.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 11 = 0.07247877514659831\n",
      "accuracy on epoch 11 = 0.9804947668886774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:28<00:00, 37.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 12 = 0.05466890645306026\n",
      "accuracy on epoch 12 = 0.9835870599429115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:27<00:00, 38.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 13 = 0.0474337928500711\n",
      "accuracy on epoch 13 = 0.9871550903901046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:27<00:00, 38.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 14 = 0.033360494873031485\n",
      "accuracy on epoch 14 = 0.9897716460513797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [00:27<00:00, 38.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 15 = 0.037771118129498335\n",
      "accuracy on epoch 15 = 0.9883444338725024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs=15\n",
    "for epoch in range(epochs):\n",
    "    time.sleep(1)\n",
    "    total_loss = 0.0\n",
    "    total_acc=0.0\n",
    "    for i, batch in enumerate(tqdm(trainset)):\n",
    "        feature, label = batch['token_list'].to(device), batch['label'].to(device)\n",
    "#         batch_length = torch.tensor(33, dtype = torch.int64).unsqueeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        output =  model(feature).squeeze()\n",
    "        loss = loss_function(output, label)\n",
    "        acc=categorical_accuracy(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc+=acc.item() \n",
    "        \n",
    "\n",
    "    print(f\"loss on epoch {epoch+1} = {total_loss/len(trainset)}\")\n",
    "    print(f\"accuracy on epoch {epoch+1} = {total_acc/len(trainset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3603adb4-1961-4f9e-88dd-fe7864c284e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "itol = {0: 'Hate Speech', 1: 'Offensive', 2: 'Neither'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed8aa734-ab8e-4e73-9591-442f1865e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(model, sent, min_len = 4):\n",
    "    model.eval()\n",
    "    sent =  re.sub(r'RT','',sent)\n",
    "    sent =  re.sub('@[a-zA-z0-9_]+','',sent)\n",
    "    sent =  re.sub(r'http://[^\\s<>\"]+|www\\.[^\\s<>\"]+','',sent)\n",
    "    sent =  re.sub(r'&#[0-9]+','',sent)\n",
    "    sent =  re.sub(r'[0-9]+','',sent)\n",
    "    sent =  re.sub(r'[^\\w\\s]', '',sent)\n",
    "    sent =  sent.strip()\n",
    "    sent =  \" \".join(sent.split())\n",
    "    sent = [token.lemma_ for token in nlp(sent) if token.text not in STOP_WORDS]\n",
    "    vector = []\n",
    "    for token in sent:\n",
    "        vector.append(vocab.lookup_token(token))\n",
    "    tensor = torch.LongTensor(vector)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    preds = model(tensor.to(device))\n",
    "    pred_class = preds[0].argmax(dim = 1)\n",
    "    \n",
    "#     print('The sentence is : {}'.format(sent))\n",
    "    print(f'Predicted class is: {pred_class.item()} = {itol[int(pred_class)]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04d9a105-8f4b-4aec-b7e0-fd6d49dbbae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is: 2 = Neither.\n",
      "Predicted class is: 0 = Hate Speech.\n"
     ]
    }
   ],
   "source": [
    "predict_class(model=model, sent=\"Hate is a strong word\")\n",
    "predict_class(model=model, sent=\"Hate is a strong word. I would use it to describe my feelings towards noun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30b9e3b9-a2c0-4fde-bbc4-966f2eb82d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is: 2 = Neither.\n"
     ]
    }
   ],
   "source": [
    "test_sen = 'What a wonderful world'\n",
    "predict_class(model, test_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc6d74-f81c-48f6-982c-6c212730ef77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
